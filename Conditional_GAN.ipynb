{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPH8G+SBCgqIZMDK/OeWtHJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarveshD7/Conditional-GAN-Pytorch/blob/main/Conditional_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AdO9LBGbhYrM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LEARNING_RATE = 5e-5\n",
        "BATCH_SIZE = 64\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 1\n",
        "NUM_CLASSES = 10\n",
        "GEN_EMBEDDING = 100\n",
        "Z_DIM = 100\n",
        "NUM_EPOCHS = 5\n",
        "FEATURES_critic = 64\n",
        "FEATURES_GEN = 64\n",
        "CRITIC_ITERATIONS = 5\n",
        "WEIGHT_CLIP = 0.01"
      ],
      "metadata": {
        "id": "A4z3kgsYwHtu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eWEKagld5Yla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea is to add an additional channel showing the label of the image so that the required output according to the condition can be generated"
      ],
      "metadata": {
        "id": "mqplLBA6xE17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_penalty(critic,labels, real, fake, device=\"cpu\"):\n",
        "  BATCH_SIZE, C, H, W = real.shape\n",
        "  epsilon = torch.rand((BATCH_SIZE,1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "  interpolated_images = real*epsilon + fake*(1-epsilon)\n",
        "\n",
        "  #  Calculate Critic scores\n",
        "  mixed_scores = critic(interpolated_images, labels)\n",
        "  gradient = torch.autograd.grad(\n",
        "      inputs=interpolated_images,\n",
        "      outputs=mixed_scores,\n",
        "      grad_outputs = torch.ones_lime(mixed_scores),\n",
        "      create_graph = True,\n",
        "      retain_graph = True\n",
        "  )[0]\n",
        "\n",
        "  gradient = gradient.view(gradient.shape[0], -1)\n",
        "  gradient_norm = gradient.norm(2, dim=1)\n",
        "  gradient_penalty = torch.mean((gradient_norm- 1)**2)\n",
        "  return gradient_penalty"
      ],
      "metadata": {
        "id": "_bqoAG-947Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, channels_img, features_d, num_classes, img_size):  # features_d is channels that are going to change in different layers\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.img_size = img_size\n",
        "    self.critic = nn.Sequential(\n",
        "        # Input: N x channels_img x 64 x 64\n",
        "        nn.Conv2d(channels_img+1, features_d, kernel_size=4, stride=2, padding=1),  # 32x32\n",
        "        nn.LeakyReLU(0.2),\n",
        "        self._block(features_d, features_d*2, 4,2,1),  # 16x16\n",
        "        self._block(features_d*2, features_d*4, 4,2,1),  # 8x8\n",
        "        self._block(features_d*4, features_d*8, 4,2,1),  # 4x4\n",
        "        nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0), # 1x1\n",
        "    )\n",
        "    self.embed = nn.Embedding(num_classes, img_size*img_size)\n",
        "    # The nn.Embedding layer is a simple lookup table that maps an index value to a weight matrix of a certain dimension\n",
        "  def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "        nn.InstanceNorm2d(out_channels, affine=True),  # LayerNorm <----> InstanceNorm\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "\n",
        "  def forward(self,x, labels):\n",
        "    embedding = self.embed(labels).view(labels.shape[0], 1, self.image_size, self.img_size)\n",
        "    x = torch.cat([x, embedding], dim=1)  # N x C x img_size x img_size\n",
        "\n",
        "    return self.critic(x)\n"
      ],
      "metadata": {
        "id": "ORyQPqJ-wPeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, z_dim, channels_img, features_g, num_classes, img_size, embed_size):\n",
        "    super(Generator, self).__init__()\n",
        "    self.img_size = img_size\n",
        "    # Input: N x z_dim x 1 x 1\n",
        "    self.gen = nn.Sequential(\n",
        "        self._block(z_dim+embed_size, features_g*16, 4, 1, 0), # 4x4\n",
        "        self._block(features_g*16, features_g*8, 4, 2, 1), # 8x8\n",
        "        self._block(features_g*8, features_g*4, 4, 2, 1), # 16x16\n",
        "        self._block(features_g*4, features_g*2, 4, 2, 1), # 32x32\n",
        "        nn.ConvTranspose2d(features_g*2,channels_img, kernel_size=4, stride=2, padding=1),  # 64x64\n",
        "        nn.Tanh()  # Range = [-1,1]\n",
        "    )\n",
        "    self.embed = nn.Embedding(num_classes, embed_size)\n",
        "  def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "        # nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x, labels):\n",
        "    embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
        "    x = torch.cat([x, embedding], dim=1)\n",
        "    return self.gen(x)\n"
      ],
      "metadata": {
        "id": "a3YCRWYCzROi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing weights of the model with a normal distribution with given mean and standard deviation\n",
        "def initialize_weights(model):\n",
        "  for m in model.modules():\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "      nn.init.normal_(m.weight.data, 0.0, 0.02)"
      ],
      "metadata": {
        "id": "jcCg8C7o45fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = torchvision.transforms.Compose(\n",
        "   [ torchvision.transforms.Resize(IMAGE_SIZE),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(\n",
        "        [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)],\n",
        "    )]\n",
        ")"
      ],
      "metadata": {
        "id": "9k-uYOSa5dmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.MNIST(root=\"/content/dataset/\", train=True, transform=transforms, download=True)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "XQrpyPvn5fwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMAGE_SIZE, GEN_EMBEDDING).to(device)\n",
        "critic = Discriminator(CHANNELS_IMG, FEATURES_critic, NUM_CLASSES,IMAGE_SIZE).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(critic)"
      ],
      "metadata": {
        "id": "IRGXQRbp5fvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LAMBDA_GP = 10\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "# criterion = nn.BCELoss()  Not needed in WGAN\n",
        "\n",
        "# Wrting the training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print(f\"Started epoch-> {epoch}\")\n",
        "  for batch_idx, (real, labels) in enumerate(loader):\n",
        "    real = real.to(device)\n",
        "    labels = labels.to(device)\n",
        "    # Train the Discriminator (Critic) CRITIC_ITERATIONS time for every time Generator is trained\n",
        "    for _ in range(CRITIC_ITERATIONS):\n",
        "      noise = torch.randn((BATCH_SIZE, Z_DIM, 1, 1)).to(device)\n",
        "      fake = gen(noise)\n",
        "      critic_real = critic(real, labels).reshape(-1)\n",
        "      critic_fake = critic(fake, labels).reshape(-1)\n",
        "      gp = gradient_penalty(critic, labels, real, fake, device=device)\n",
        "      loss_critic = (\n",
        "          -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP*gp\n",
        "      )\n",
        "      critic.zero_grad()\n",
        "      loss_critic.backward(retain_graph=True)  # retain_graph=True since we want to reuse fake and Pytorch erases intermediate results on loss.backward()\n",
        "      opt_critic.step()\n",
        "\n",
        "    # Training the Generator\n",
        "    output = critic(fake, labels).reshape(-1)\n",
        "    loss_gen = -torch.mean(output)\n",
        "    gen.zero_grad()\n",
        "    loss_gen.backward()\n",
        "    opt_gen.step()\n",
        "  print(f\"Completed epoch-> {epoch}\")\n"
      ],
      "metadata": {
        "id": "tw1aPo6P56_p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}